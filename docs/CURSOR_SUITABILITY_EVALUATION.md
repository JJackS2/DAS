# Cursor 적합성 평가 방법

본 문서는 **Cursor에 얼마나 적합하게 프로젝트가 구성되었는지**를 상세히 평가하기 위한 **평가 기준**, **평가 방법**, **점수 산출** 방식을 정의합니다. 구성 여부 확인 및 내부 품질 평가에 사용합니다.

---

## 0. 점수 기준이 무엇인가 / 왜 “높게 만족하면 잘 만들어진 것”으로 보는가

### 0.1 점수 기준의 출처

- **기준 자체**: 아래 §2의 5개 영역(규칙·스킬·문서·명령·검증)과 각 세부 항목·배점은, Cursor 공식 블로그(agent best practices), Cursor Docs(Rules, Skills, Commands), Developer Toolkit(Setup/Rules), 본 프로젝트 CURSOR_EFFICIENCY_GUIDE에서 권장하는 항목을 **프로젝트에서 제어 가능한 것**만 골라 정리한 것이다.
- **배점 비율**: 규칙·스킬·문서는 “에이전트가 맞춤으로 동작하는 데 직접 영향”이 크므로 각 25점; 명령·워크스페이스·인덱싱은 “반복 워크플로·성능”에 기여하므로 15점; 검증 가능 목표는 “에이전트가 올바름을 판단할 수 있게 함”이므로 10점으로 두었다.
- **항목별 배점(5점 단위)**: 한 영역 안에서 세부 항목을 균등하게 나누어, 체크리스트로 확인·감점하기 쉽게 5점 단위로 둔다.

### 0.2 “점수를 높게 만족하면 잘 만들어진 것”이라고 말하는 근거

- **정의**: 본 프로젝트에서 **“Cursor 적합성이 잘 만들어진 것”**이란, “에이전트가 규칙·스킬·문서를 참조해 **맥락을 알고**, **반복 워크플로를 명령으로 실행**하며, **검증 가능한 목표(E2E·계약)**로 올바름을 확인할 수 있는 상태”를 말한다.
- **왜 90점 이상을 “잘 만들어진 것”으로 두었는가**:  
  - 90점은 100점 중 **한 영역 전체(예: 인덱싱 5점) 정도가 비어 있어도**, 나머지가 충족되면 “에이전트가 상세하게 사용하기에 필요한 맥락·워크플로·검증”은 갖춰진 것으로 보자는 **운영상의 기준선**이다.  
  - 즉, “이론적 완벽(100점)”이 아니라 “실제로 Cursor를 상세 사용할 때 **장애가 되지 않는 수준**”을 90으로 둔 것이다.  
  - 75·60·0 구간은 “일부만 갖춤” → “많이 부족” → “거의 없음”으로 해석하면 된다.
- **재정의 가능성**: 팀이 “잘 만들어진 것”의 정의나 90/75/60 구간을 바꾸고 싶으면, 이 §0.2 문단을 수정해 **프로젝트 고유의 기준**으로 덮어쓰면 된다.

---

## 1. 평가 목적

- **구성 여부 확인**: 규칙·스킬·명령·문서·인덱싱 등이 Cursor 활용에 필요한 수준으로 갖춰졌는지 확인.
- **내부 품질 평가**: 각 항목이 “상세하게 사용하기에” 충분한 수준으로 작성되었는지 평가.
- **개선 방향**: 점수·미달 항목을 바탕으로 우선 개선할 부분을 선정.

---

## 2. 평가 영역 및 세부 항목

### 2.1 규칙 (Rules) — 25점 만점

| 항목 | 배점 | 평가 기준 | 확인 방법 |
|------|------|-----------|-----------|
| **파일 패턴(globs)** | 5 | 모든 규칙이 적용 대상을 globs로 명시하고, 중복·공백 없음 | .cursor/rules/*.mdc frontmatter 확인 |
| **작업별 스킬 명시** | 5 | “이 작업 시 이 스킬 사용”이 규칙 본문에 표 또는 목록으로 있음 | 규칙 본문에 스킬 표/목록 존재 여부 |
| **한 관심사 한 규칙** | 5 | 규칙당 한 가지 관심사(명세/개발/테스트 등), 50줄 이하 권장 | 규칙 개수·내용·줄 수 |
| **캐노니컬 참조** | 5 | DEVELOPMENT_RULES, TEST_RULES, 명세 등 파일 경로로 참조(내용 복붙 최소화) | 규칙 내 “참조” 문구 |
| **description 품질** | 5 | description이 구체적이고, 에이전트가 언제 적용할지 판단 가능 | frontmatter description 검토 |

### 2.2 스킬 (Skills) — 25점 만점

| 항목 | 배점 | 평가 기준 | 확인 방법 |
|------|------|-----------|-----------|
| **description(WHAT+WHEN)** | 5 | “무엇을 하는지”와 “언제 사용할지”가 description에 포함 | SKILL.md frontmatter |
| **단일 책임** | 5 | 세분화 스킬은 한 가지 워크플로만, 오케스트레이션은 순서만 | 스킬 본문 범위 |
| **500줄 이하·참조 1단계** | 5 | SKILL.md 500줄 이하, reference 등 참조는 1단계 깊이 | 줄 수·링크 깊이 |
| **트리거 용어** | 5 | 한국어/영어 트리거 용어가 description에 포함 | description 키워드 |
| **다음 단계 명시** | 5 | “다음 단계” 또는 “다음 스킬”이 명시되어 워크플로 연결 | 본문 내 “다음” 문구 |

### 2.3 문서·아키텍처 — 25점 만점

| 항목 | 배점 | 평가 기준 | 확인 방법 |
|------|------|-----------|-----------|
| **프로젝트 전체 아키텍처** | 5 | 데이터 흐름·시스템 경계·알고리즘 개요가 한 문서에 있음 | docs/ARCHITECTURE.md 등 존재·내용 |
| **모듈 수준 설명** | 5 | web/server/treemap/tests 각각 역할·입출력·의존성 문서화 | 모듈별 README 또는 ARCHITECTURE 내 섹션 |
| **알고리즘 설명** | 5 | buildTree 집계·품질 지표·트리맵 레이아웃 등 핵심 알고리즘 설명 | 문서 내 “알고리즘”/“로직” 섹션 |
| **계약·ID·선택자** | 5 | API·state·DOM 계약이 명세 또는 DEVELOPMENT/TEST_RULES에 정리 | 명세 §3·§4, TEST_RULES §4 |
| **루트 맥락(AGENTS.md 등)** | 5 | 저장소 루트에서 “이 프로젝트는 무엇인지, 규칙은 어디 있는지” 한 페이지로 파악 가능 | AGENTS.md 또는 동등 문서 |

### 2.4 명령·워크스페이스·인덱싱 — 15점 만점

| 항목 | 배점 | 평가 기준 | 확인 방법 |
|------|------|-----------|-----------|
| **슬래시 명령** | 5 | 반복 워크플로(테스트·빌드·동기화)가 .cursor/commands에 2개 이상 | .cursor/commands/*.md 개수·내용 |
| **워크스페이스** | 5 | 멀티루트 워크스페이스로 web/server/docs/tests 동시 편집 가능 | .code-workspace 존재·folders |
| **인덱싱 제외** | 5 | .cursorignore 또는 search.exclude로 node_modules·dist·build·reports 제외 | .cursorignore·workspace settings |

### 2.5 검증 가능한 목표 — 10점 만점

| 항목 | 배점 | 평가 기준 | 확인 방법 |
|------|------|-----------|-----------|
| **E2E 테스트** | 5 | Playwright 등으로 “동작 정상 여부”를 자동 판정 가능 | tests/treemap/*.spec.ts, npm run test |
| **계약 문서화** | 5 | 테스트가 의존하는 id/data-testid/__TREEMAP_DEBUG__가 문서와 일치 | TEST_RULES §4, DEVELOPMENT_RULES §5 |

---

## 3. 점수 산출 및 등급

- **총점**: 위 5개 영역 합계 **100점**.
- **항목별**: 각 세부 항목을 0/배점(부분 점수 가능)으로 부여. “전부 충족”이면 만점, “일부만”이면 비율로 감점.
- **등급**:
  - **90–100**: 아주 적합 (Excellent)
  - **75–89**: 적합 (Good)
  - **60–74**: 보통 (Fair)
  - **0–59**: 개선 필요 (Needs improvement)

---

## 4. 평가 실행 방법

### 4.1 수동 체크리스트

1. **규칙**: .cursor/rules/ 내 각 .mdc를 열어 2.1 표대로 globs·스킬 표·줄 수·description 확인.
2. **스킬**: .cursor/skills/ 내 각 SKILL.md를 열어 2.2 표대로 description·단일 책임·줄 수·트리거·다음 단계 확인.
3. **문서**: docs/ 및 finviz-like-treemap/docs/에서 아키텍처·모듈·알고리즘·계약·AGENTS.md 존재 여부 확인.
4. **명령·워크스페이스·인덱싱**: .cursor/commands/, DAS.code-workspace, .cursorignore(또는 PROJECT_GUIDE 템플릿 적용 여부) 확인.
5. **검증**: tests/treemap/ 및 TEST_RULES·DEVELOPMENT_RULES 대조.

### 4.2 점수 기록

- `docs/CURSOR_SUITABILITY_SCORE.md`(또는 본 문서 하단)에 **평가 일자**, **영역별 점수**, **총점**, **등급**, **미달 항목·개선 권장**을 기록.
- 개선 후 재평가하여 전후 비교.

### 4.3 정기 평가

- 규칙·스킬·명령·문서를 크게 바꾼 뒤 또는 분기 1회, 위 체크리스트로 재평가 권장.

---

## 5. 본 프로젝트에의 적용

- **구성 여부**: 위 2.1~2.5 항목이 “있음/문서화됨”인지로 구성 완료 여부 판단.
- **내부 품질**: “상세하게 사용하기 위한” 수준은 “모듈 수준 설명·알고리즘 설명·계약 정리”가 되어 있는지로 판단.
- **적합성**: 총점·등급으로 “얼마나 적합하게 작성되었는가”를 정량화.

최초 평가 결과 및 개선 이력은 동일 문서 또는 `CURSOR_SUITABILITY_SCORE.md`에 누적합니다.
